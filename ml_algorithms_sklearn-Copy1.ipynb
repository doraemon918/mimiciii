{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data from the 50-50 readmit vs. nonreadmit dataset\n",
    "\n",
    "df = pd.read_csv('dfd.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping certain columns\n",
    "\n",
    "# Dropping ID numbers and dates\n",
    "df = df.drop(columns=['subject_id', 'hadm_id', 'admittime', 'dischtime'])\n",
    "\n",
    "# Dropping labevents and chartevents values of less importance based on feature selection (2/3 values for each measurement)\n",
    "df = df.drop(columns=['rdw_min', 'rdw_max', 'hemoglobin_min', 'hemoglobin_max', 'creatinine_median', 'creatinine_min', \n",
    "                      'hematocrit_median', 'hematocrit_min', 'tempc_median', 'tempc_max', 'resprate_median', \n",
    "                      'resprate_min', 'wbc_median', 'wbc_max', 'inr_min', 'inr_median', 'ptt_median', 'ptt_max', \n",
    "                      'lactate_median', 'lactate_max', 'sysbp_median', 'sysbp_min', 'spo2_median', 'spo2_max', \n",
    "                      'bilirubin_median', 'bilirubin_max', 'platelet_median', 'platelet_max', 'heartrate_min',\n",
    "                      'heartrate_median'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical features into dummy variables\n",
    "\n",
    "df_converted = pd.get_dummies(df)\n",
    "df_converted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataframe into data (predictors) vs. label (attributed to be predicted)\n",
    "\n",
    "label_df = df_converted.pop('followed_by_readmit')\n",
    "data_df = df_converted\n",
    "print('label_df:\\n', label_df.head(), 2*'\\n', 'data_df:\\n', data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataframes to NumPy arrays\n",
    "\n",
    "label = label_df.values\n",
    "data = data_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 train-test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_label, test_label = train_test_split(data, label, train_size=0.8, test_size=0.2, random_state=10)\n",
    "\n",
    "print('Training data:', train_data.shape, '\\tTest data:', test_data.shape)\n",
    "print('Training labels:', train_label.shape, '\\tTest labels:', test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on the training set (no need to do explicitly for Logistic Reg. since has a cv parameter, but just in case...)\n",
    "\n",
    "#from sklearn.model_selection import KFold\n",
    "#kf = KFold(n_splits=5, random_state=10)\n",
    "#for train, test in kf.split(train_data, train_label):\n",
    "    #print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that cross-validation will be performed with the training data for each machine learning algorithm tested below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression:\n",
    "\"LogisticRegressionCV implements Logistic Regression with builtin cross-validation to find out the optimal C parameter (similar to what GridSearchCV might do).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try using the `liblinear` solver (recommended for small datasets) using 'l1' penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating logistic regression estimator object; cv=5 indicates 5-fold cross-validation\n",
    "\n",
    "lrcv_l1 = LogisticRegressionCV(cv=5, penalty='l1', solver='liblinear', random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lrcv_l1.fit(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv_l1_pred = lrcv_l1.predict(test_data)\n",
    "lrcv_l1_pp = lrcv_l1.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running self.classes_ on the result from fitting the model to the data (see commented code below) tells us that the probability estimates for positive class (\"True\") are in column 1 rather than column 0 of the result from `predict_proba()` - this is what we'll need to use when calculating the ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrcv_l1.fit(train_data, train_label).classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "lrcv_l1_probs = lrcv_l1.predict_proba(test_data)[:,1]\n",
    "print('ROC-AUC score with l1 penalty:', roc_auc_score(test_label, lrcv_l1_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we used 'l2' penalty instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Repeating the code above for logistic regression but with l2 penalty\n",
    "\n",
    "lrcv_l2 = LogisticRegressionCV(cv=5, penalty='l2', solver='liblinear', random_state=10)\n",
    "lrcv_l2.fit(train_data, train_label)\n",
    "lrcv_l2_pred = lrcv_l2.predict(test_data)\n",
    "lrcv_l2_pp = lrcv_l2.predict_proba(test_data)\n",
    "lrcv_l2_probs = lrcv_l2.predict_proba(test_data)[:,1]\n",
    "print('ROC-AUC score with l2 penalty:', roc_auc_score(test_label, lrcv_l2_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the liblinear solver, l1 penalty has a very slightly better ROC-AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the confusion matrix and precision/recall/F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_label, lrcv_l1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_label, lrcv_l1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
